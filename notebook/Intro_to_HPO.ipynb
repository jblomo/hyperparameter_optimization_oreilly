{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hyperparameter Optimization\n",
    "\n",
    "## Discussion of HPO principles and collaboration on an interactive use case\n",
    "\n",
    "`The tutorial will be run by Jim Blomo (jim@sigopt.com)`\n",
    "\n",
    "\n",
    "`Original content created by Jeremy Bivaud (jeremy@sigopt.com) and Tobias Andreasen (tobias@sigopt.com)`\n",
    "\n",
    "___\n",
    "\n",
    "## Introduction\n",
    "\n",
    "*Experimentation is critical to developing models but can be a messy process. Modelers often spend significant time on tasks like tracking runs, creating visualizations, and troubleshooting hyperparameter optimization jobs, all of which could be supported or automated with software.*\n",
    "\n",
    "*Join expert Jim Blomo to learn best practices for tracking, training, and tuning models and using the information from these processes to make the best decisions around the model development process. Youâ€™ll focus specifically on hyperparameter optimization (HPO): selecting the best method, executing tuning jobs, and analyzing the results of these jobs to select the best model for production. Along the way, youâ€™ll see firsthand just how useful HPO is through an anomaly detection problem (based on a Kaggle financial dataset) that uses an XGBoost classification model. Youâ€™ll then use SigOpt to perform your own tuning jobs and cover open source alternatives and how to implement them* - [O'Reilly](https://www.oreilly.com/live-training/courses/introduction-to-hyperparameter-optimization/0636920470830/#instructors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructor: Jim Blomo\n",
    "\n",
    "*Jim Blomo is an executive engineering leader at SigOpt. Heâ€™s achieved strong business results in technology companies by creating a culture of performance, innovation, and teamwork. Previously, Jim led data-mining efforts as a director of engineering at Yelp, operations at the startup PBWorks, and search infrastructure at Amazonâ€™s A9 subsidiary. He enjoys speaking and travel; heâ€™s lectured on data mining and web architecture at UC Berkeley's School of Information and presented at conferences such as AWS re:Invent, Oâ€™Reilly OSCON, Wolfram Data Summit, and RecSys. He loves exploring the food and outdoors of the Bay Area with his family* - [Jim Blomo](https://www.linkedin.com/in/jimblomo/)\n",
    "\n",
    "<img src=\"https://sigopt.com/wp-content/uploads/2019/02/img-Jim.jpg\" alt=\"Black Box Overview\"  width=\"200\" align=\"center\"/> \n",
    "\n",
    "<img src=\"https://static.sigopt.com/b/ac75899cedb91b80acf515ed92fef03aa4ef690d/static/img/SigOpt_logo_horiz.svg\" alt=\"Black Box Overview\" width=\"300\"/>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmUeuv7a1n5E"
   },
   "source": [
    "# Session Abstract\n",
    "\n",
    "This sessionâ€™s primary objective is to teach attendees how to balance model training and hyperparameter tuning to develop high-performing models. Its secondary objective is to dive deeper into hyperparameter tuning, and, through the process, help attendees develop some intuition around the practical aspects of automated model optimization.\n",
    "\n",
    "> First, we will spend time discussing the best ways to track runs as you go through the modeling process. Second, we will discuss useful visualizations for analyzing model behavior, comparing architectures, and evaluating metrics. Finally, we will delve into methods for automated hyperparameter optimization with a focus on how tuning hyperparameters boosts model performance, provides model insights, and bolsters modeling workflows and team collaboration.\n",
    "\n",
    "We will present an anomaly detection problem based on this [Kaggle financial dataset](https://www.kaggle.com/ntnu-testimon/paysim1) using a [XGBoost classification model](https://xgboost.readthedocs.io/en/latest/) to show rather than tell how HPO can be useful. After presenting the use case and the dataset, weâ€™ll dive into the optimization journey, presenting the model performance and workflow uplifts that a modeler can observe when following a structured optimization approach using SigOpt. The session will feature multiple interactive sections including code exercises and group discussions that will utilize Jupyter notebooks. Every attendee will get free access to [SigOpt](https://sigopt.com/) so they can perform their own tuning jobs as part of the tutorial. We will also cover open source alternatives to SigOpt and train attendees on how to integrate them if they need a replacement for SigOpt.\n",
    "\n",
    "\n",
    "ðŸš¦ **TO DO [RECOMMENDED] - Sign up for a free SigOpt account in order to follow along during the tutorial - [SIGN UP FOR SIGOPT](https://modeling.sigopt.com/oreilly-offer).**\n",
    "\n",
    "After signing up for the SigOpt account you will recieve an email with instructions on how to activate your personal SigOpt account.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWQDFfjR1n5F"
   },
   "source": [
    "# Session Agenda\n",
    "\n",
    "This session is divided in three sections;\n",
    "\n",
    "* [__Data Import and preprocessing__](#Data-Import-and-Preprocessing) (20 minutes)\n",
    "    * Modeling Environment\n",
    "    * Importing Libraries\n",
    "    * Importing the Dataset\n",
    "    * Defining our Feature and Label Sets\n",
    "    * Objective Metric Selection\n",
    "    * Splitting the dataset\n",
    "* [__Experiment Tracking__](#Experiment-Tracking) (30 minutes)\n",
    "    * Experiment Tracking with SigOpt\n",
    "        * Training Runs\n",
    "        * Experiments\n",
    "    * Setting Up SigOpt\n",
    "    * Setting our baseline\n",
    "* [__Hyperparameter Optimization__](#Hyperparameter-Optimization) (75 minutes)\n",
    "    * Define your Parameter Space\n",
    "    * Configure your Experiment\n",
    "    * Instrument your model and run your Experiment\n",
    "    * Multimetric Experimentation\n",
    "* [__Learn more at your own time__](#Learn-more-at-your-own-time)\n",
    "    * EfficientBERT\n",
    "    * Advanced features\n",
    "\n",
    "There will be a 5 min break between the Experiment Tracking and Hyperparameter Optimization sections. Let's get started!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Preprocessing\n",
    "\n",
    "## Modeling Envirnment\n",
    "\n",
    "In the interest of time we have already prepared the environment that we'll be using for this tutorial. All of the libraries listed belowe have been install using *pip install*'.\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell to have a look at the environment that we will be using for the tutorial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../requirements.txt', 'r') as txt:\n",
    "    print('Preinstalled libraries: \\n')\n",
    "    print(txt.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "All of the above libraries have bee pre-installed and are ready to be imported.\n",
    "\n",
    "Links and a short description of why each of these libraries are important for the tutorial can be found below.\n",
    "- **pandas** to load the data file as a Pandas dataframe, analyze and process the data directly within the notebook\n",
    "- **numpy** and **math** for computing scientific functions\n",
    "- **time** to measure inference and training time\n",
    "- **SigOpt** for experimentation and optimization\n",
    "- from **sklearn**, we'll import [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split our dataset into train and test subsets, [average_precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html), [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) to track our model performance\n",
    "- and finally, from **xgboost**, we'll import the [XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier) class that we will use to build and parametrize our model\n",
    "\n",
    "We also use a random_state variable across this notebook to guarantee that all our functions are deterministic, and results are repeatable.\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell to import the required libraries and classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkcWiyaK1n5H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import sigopt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, f1_score \n",
    "from xgboost.sklearn import XGBClassifier\n",
    "random_state = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rt5qDTFa1n5L"
   },
   "source": [
    "## Importing the Dataset\n",
    "The dataset we are using is a synthetic dataset generated using the [PaySim](https://github.com/EdgarLopezPhD/PaySim) simulator. PaySim uses aggregated data from a private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behavior to later evaluate the performance of fraud detection methods. The dataset is publicly available under this kaggle [project](https://www.kaggle.com/ntnu-testimon/paysim1). For your convenience we performed the data preprocessing and feature engineering on the dataset. We also picked a 10% subset of this data so we can iterate fast and try enough sets of hyperparameters to showcase the optimization process and still fit this journey within a two hour interactive session.\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell to import the data set and create the corresponding Pandas dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kHQ87NyY1n5L",
    "outputId": "d91fd3f7-a6fe-4d40-b174-ac9be38f4cb4"
   },
   "outputs": [],
   "source": [
    "df_path = '../data/Fraud_Detection_SigOpt_dataset.csv'\n",
    "df = pd.read_csv(df_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IafCRqoO1n5P"
   },
   "source": [
    "## Defining our Feature and Label Sets\n",
    "ðŸš¦ **TO DO - Run the following code cell to split our dataset into the feature set X and the label set Y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGKkkLy81n5P"
   },
   "outputs": [],
   "source": [
    "X = df\n",
    "Y = df['isFraud']\n",
    "del X['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIax6nK71n5T"
   },
   "source": [
    "In most Anomaly Detection problems, the main obstacle for training a robust ML model is the highly imbalanced nature of the data.  The formula below returns a measure of the dataset skew, which is is the share of fraudulent transactions in our dataset which is a little more than .1%\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell to find out the share of fraudulent activity in your dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LCc23O8J1n5U",
    "outputId": "1e770585-bdc8-48ab-c118-d488e59dc187"
   },
   "outputs": [],
   "source": [
    "print('Share of fraudulent activity: {}%'.format(100*(len(Y.loc[Y == 1]) / float(len(Y)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Metric Selection\n",
    "\n",
    "Now that we have established that the dataset is highly skewed towards one of our classes. It is important to pick objective metric(s) that is able to account for this type of class imbalance. Oftentimes this becomes a question of what represents success for your application. For this tutorial we will focus on some common objective metrics used for imbalanced dataset:\n",
    "\n",
    "- **F1-score**, **precision** and **recall**: F1 is a wieghted average of precision and recell and tryes to account for one class having a larger representation than the other.\n",
    "- **Area under the precision-recall curve (AUPRC)** and **area under the receiver operating characteristic (AUROC)**: these do have similar charaacteristics but oftentime (AUPRC) does a better job weighing the bad prediction in the minority class versus a metric like the AUROC. More info on this tradeoff in this [publication](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf).\n",
    "- **Confusion matrix**: the confusion matric gives an indication of how the 'wrong' predictions falls.\n",
    "\n",
    "Given that it is important to come up with objective metrics that represents success for your business, we have designed two metrics to be used for this tutorial:\n",
    "\n",
    "- **error_fraud( )**: record the mean transaction amount from missing fraudulent transactions\n",
    "- **error_valid( )**: record the mean transaction amount from flagging valid transactions\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cells to call these two objective metrics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_fraud(prediction, label, amount):\n",
    "    \"\"\"record the mean transaction amount from missing fraudulent transactions\"\"\"\n",
    "    fn_vec = (~prediction) & (label > 0)\n",
    "    fraud_loss_mean = (np.where(fn_vec, np.abs(amount), np.zeros_like(amount)).sum() / (label > 0).sum())\n",
    "    return fraud_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_valid(prediction, label, amount):\n",
    "    \"\"\"record the mean transaction amount from flagging valid transactions\"\"\"\n",
    "    fp_vec = prediction & (label == 0)\n",
    "    valid_loss_mean = (np.where(fp_vec, np.abs(amount), np.zeros_like(amount)).sum() / (label == 0).sum())\n",
    "    return valid_loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgKGI2SV1n5Y"
   },
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "It is important, that we produce a model which is able to generalize to unseen data. In order to do so, we split our full dataset into a training set (80% of the data) and a testing set (20% of the data).\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell create your training and test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDURk10k1n5Y"
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.2, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to notice that SigOpt is not limited to using a train/test-split for guaranteeing generalization. It is perfectly resonable to use things like k-fold, simulation etc. Ultimately it comes down to, what you feel the most comfortable with. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtLno2Ml1n5c"
   },
   "source": [
    "# Experiment Tracking\n",
    "\n",
    "Modeling is messy and it can be hard to keep track of everything. With just a few lines of code, SigOpt [Experiment Management](https://sigopt.com/experiment-management/) helps you track and organize your training and tuning cycles, including: architectures, metrics, parameters, hyperparameters, code snapshots and the results of feature analysis, training runs or tuning jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNkn32WF1n5d"
   },
   "source": [
    "## Experiment Tracking with SigOpt\n",
    "\n",
    "In the previous section, we loaded our dataset in a pandas dataframe and ran all the prerequirements to start experimenting, including hyperparameter optimization. Let's go over some SigOpt specific terminology.\n",
    "<img src=\"https://static.sigopt.com/b/fdc2afa8b676e4bdc59e92267455bd24c14e1376/static/img/docs/black_box_overview.png\" alt=\"Black Box Overview\"  width=\"700\"/>\n",
    "SigOpt is a platform and framework agnostic experiment management and optimization platform. As you use SigOpt, you'll regularly be creating either [Training Runs](https://app.sigopt.com/docs/runs/overview), or\n",
    "tuning [Experiments](https://app.sigopt.com/docs/overview/create). Both of these are organized within a Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEOBwhKB1n5e"
   },
   "source": [
    "### Training Runs\n",
    "A SigOpt Run stores the training and evaluation of a model, so that modelers can see a history of their work. This is the fundamental building block of SigOpt. Runs record everything you might need to understand how a model was built, reconstitute the model in the future, or explain the process to a colleague.\n",
    "\n",
    "Common attributes of a Run include:\n",
    "- the model type,\n",
    "- dataset identifier,\n",
    "- evaluation metrics,\n",
    "- hyperparameters,\n",
    "- logs, and\n",
    "- a code reference.\n",
    "\n",
    "For a complete list of attributes see the [API Reference](https://app.sigopt.com/docs/runs/reference). Training runs can be recorded by integrating code snippets into Python that you run in a [notebook](https://app.sigopt.com/docs/runs/notebook) or via the [command line](https://app.sigopt.com/docs/runs/editor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY4lFBNo1n5e"
   },
   "source": [
    "### Experiments\n",
    "An experiment is an automated search of your model's hyperparameter space, sometimes for the purpose of tuning a model. You define the parameter space and request [suggestions](https://app.sigopt.com/docs/objects/suggestion) from the SigOpt API. Suggestions can be generated randomly, as a grid search, or using an ensemble of Bayesian optimization methods.\n",
    "\n",
    "Experiments can be instrumented to create one training run per suggestion. The metrics measuring the model's performance are referred to as an [observation](https://app.sigopt.com/docs/objects/observation) in the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUjYRYjb1n5j"
   },
   "source": [
    "## Setting up SigOpt\n",
    "\n",
    "Before starting to do modeling, we will need to configer the SigOpt library to connect to the SigOpt backend and come up with a name for our project.\n",
    "\n",
    "ðŸš¦ **TO DO - 1) log into your SigOpt account and access your own personal [API token](https://app.sigopt.com/tokens/info), 2) run the following code cell and 3) follow the instruction in order to connect to the SigOpt backend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = input('Insert your API token: ')\n",
    "os.environ['SIGOPT_API_TOKEN'] = API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš¦ **TO DO - 1) run the following code cell and 2) follow the instructions to name your project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joLpvdww1n5k"
   },
   "outputs": [],
   "source": [
    "%load_ext sigopt\n",
    "PROJECT_NAME = input('Name your project: ')\n",
    "os.environ['SIGOPT_PROJECT'] = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU6mxQsE1n5o"
   },
   "source": [
    "## Setting our baseline\n",
    "\n",
    "In any optimization problem, it is critical to define a baseline so any uplift in performance can be evaluated against that baseline. In our scenario the baseline is running our XGBoost classifier with all default parameters. Let's now look at three SigOpt methods we are using to build consistency in our experimentation approach\n",
    "\n",
    "- [sigopt.get_parameter](https://app.sigopt.com/docs/runs/reference#get_parameter) allows us to store our defult parameters for the baseline model on our dashboard.\n",
    "- [sigopt.log_model](https://app.sigopt.com/docs/runs/reference#log_model) stores a text value that you can use to filter your runs in the SigOpt web view. In our example, you might want to filter by Fraud_Analysis to compare models of the same use case in the web charts.\n",
    "- The most important information about a model is how it performed. With [sigopt.log_metric](https://app.sigopt.com/docs/runs/reference#log_metric) you can take advantage of SigOpt's analysis dashboard of custom charts and advanced sorting and filtering.\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell to create your baseline model, fit it, collect your performance metrics of other metrics of interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "0Fes7LBJ1n5p",
    "outputId": "8582bc7b-815b-49bd-9f01-b53de604e4de"
   },
   "outputs": [],
   "source": [
    "%%run\n",
    "sigopt.log_model('XGboost for Fraud Analysis')\n",
    "sigopt.log_dataset(df_path)\n",
    "\n",
    "model = XGBClassifier(random_state = random_state)\n",
    "\n",
    "for key, value in zip(model.get_params().keys(), model.get_params().values()):\n",
    "    sigopt.get_parameter(name=key, default=value)\n",
    "\n",
    "modelfit = model.fit(trainX,trainY)\n",
    "prediction = modelfit.predict(testX)\n",
    "F1score = f1_score(testY,prediction)\n",
    "probabilities = modelfit.predict_proba(testX)\n",
    "AUPRC = average_precision_score(testY, probabilities[:, 1])\n",
    "tn, fp, fn, tp = confusion_matrix(testY,prediction).ravel()\n",
    "\n",
    "sigopt.log_metric('AUPRC', average_precision_score(testY, probabilities[:, 1]))\n",
    "sigopt.log_metric('F1score', F1score)\n",
    "sigopt.log_metric('False Positive', fp)\n",
    "sigopt.log_metric('False Negative', fn)\n",
    "sigopt.log_metric('True Positive', tp)\n",
    "sigopt.log_metric('True Negative', tn)\n",
    "sigopt.log_metric('Mean $ Error Fraudulent', error_fraud(prediction, testY, testX['amount']))\n",
    "sigopt.log_metric('Mean $ Error Valid', error_valid(prediction, testY, testX['amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htEMEDW21n5v"
   },
   "source": [
    "ðŸš¦ **TO DO - Click the Run hyperlink above. You will be redirected to the the corresponding Run page. Now let's explore the UI and look at the data we collected during this Run**\n",
    "\n",
    "> Our AUPRC baseline is 0.96077. It is also worth noting that our baseline model only missed on 5 predictions, all False Negative (i.e. fraudulent activities that were predicted as non fraudulent) out of a total 55,585 predictions.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHgNhFT61n5x"
   },
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "The previous section illustrated how you can log and track a single run of your model by leveraging SigOpt training runs. We now want to leverage SigOpt optimization engine, and let the engine suggest sets of parameters for the purpose of tuning that same model. Similarly we'll now go over some SigOpt terminology.\n",
    "\n",
    "<img src=\"https://sigopt.com/wp-content/uploads/2019/05/SigOpt-interaction-model-1.png\" alt=\"Black Box Overview\"  width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8NZEBX11n5y"
   },
   "source": [
    "## Define your Parameter Space\n",
    "\n",
    "Today, we will explore the following parameter space\n",
    "- **min_child_weight**, used to control over-fitting, this parameter is the sample size under which the model can not split a node.  Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "- **max_depth**, this is the maximum depth of a tree.  This parameter controls over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "- **n_estimators**, this is the number of trees to fit.  Usually the higher the number of trees the better to learn the data. However, adding a lot of trees can slow down the training process and intoduce overfitting patterns.\n",
    "- **learning_rate** controls the weighting of new trees added to the model.  Lowering this value will prevent overfitting, but require the model to add a larger number of tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ_ESwJq1n5z"
   },
   "source": [
    "## Configure your Experiment\n",
    "\n",
    "The experiment definition will include the name, project, which of your parameters to optimize, metrics and other options that you would like to run your experiment with. The observation budget is the number of runs you would like created in the optimization. We recommend using 20 for testing. When you're ready, visit the [observation budget page](https://app.sigopt.com/docs/overview/observation_budget) to learn our rule of thumb for the appropriate observation budget.\n",
    "\n",
    "You can format the experiment definition in Python, YAML, or JSON. In this example, we're using YAML. See our documentation [here](https://app.sigopt.com/docs/runs/optimize#formatting) for Python and JSON examples.\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell to create your experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "1bl6qF0O1n5z",
    "outputId": "23b3106e-3f0e-4470-d941-54fdb590f042"
   },
   "outputs": [],
   "source": [
    "%%experiment\n",
    "'''\n",
    "name: Fraud_Analysis\n",
    "parameters:\n",
    "  - name: min_child_weight\n",
    "    bounds:\n",
    "      min: 1\n",
    "      max: 15\n",
    "    type: int\n",
    "  - name: max_depth\n",
    "    bounds:\n",
    "      min: 2\n",
    "      max: 15\n",
    "    type: int\n",
    "  - name: n_estimators\n",
    "    bounds:\n",
    "      min: 20\n",
    "      max: 400\n",
    "    type: int\n",
    "  - name: learning_rate\n",
    "    bounds:\n",
    "      min: 0.001\n",
    "      max: 1\n",
    "    transformation: log\n",
    "    type: double\n",
    "metrics:\n",
    "  - name: AUPRC\n",
    "    objective: maximize\n",
    "observation_budget: 20\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDDeXfP31n52"
   },
   "source": [
    "Once your experiment is created, you'll have a link to your experiment\n",
    "\n",
    "ðŸš¦ **TO DO - Click the link above to confirm your experiment was properly created.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPWjipbs1n53"
   },
   "source": [
    "## Instrument your model and run your Experiment\n",
    "\n",
    "With SigOpt, it's very easy to instrument your  model and run an optimization loop. The [sigopt.get_parameter](https://app.sigopt.com/docs/runs/reference#get_parameter) method also allows us to access the parameter suggested throughout the optimization process. In our example, the parameters that we will be optimizing are **min_child_weight**, **max_depth**, **n_estimators** and **learning_rate**. **It is important to notice that SigOpt does not require you to optimize all of your parameters, meaning that you are able to keep the defult parameters that you don't want to optimize.** When running the optimization this method will seamlessly return a value generated from a SigOpt Experiment's Suggestion.\n",
    "\n",
    "ðŸš¦ **TO DO - Run the following code cell to create the function that will generate a new model everytime SigOpt has a new set of hyperparameters to evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "guYMBJWZ1n54",
    "outputId": "da613bb0-4eff-4b18-d3c7-c964d0233862"
   },
   "outputs": [],
   "source": [
    "%%optimize\n",
    "sigopt.log_model('XGboost for Fraud Analysis')\n",
    "sigopt.log_dataset(df_path)\n",
    "\n",
    "for key, value in zip(model.get_params().keys(), model.get_params().values()):\n",
    "    sigopt.get_parameter(name=key, default=value)\n",
    "\n",
    "min_child_weight = sigopt.get_parameter('min_child_weight')\n",
    "max_depth = sigopt.get_parameter('max_depth')\n",
    "n_estimators = sigopt.get_parameter('n_estimators')\n",
    "learning_rate = sigopt.get_parameter('learning_rate')\n",
    "\n",
    "\n",
    "model = XGBClassifier(min_child_weight=min_child_weight,\n",
    "                      max_depth=max_depth,\n",
    "                      n_estimators=n_estimators,\n",
    "                      learning_rate=learning_rate,\n",
    "                      random_state = random_state)\n",
    "\n",
    "modelfit = model.fit(trainX,trainY)\n",
    "prediction = modelfit.predict(testX)\n",
    "F1score = f1_score(testY,prediction)\n",
    "probabilities = modelfit.predict_proba(testX)\n",
    "AUPRC = average_precision_score(testY, probabilities[:, 1])\n",
    "tn, fp, fn, tp = confusion_matrix(testY,prediction).ravel()\n",
    "\n",
    "sigopt.log_metric('AUPRC', average_precision_score(testY, probabilities[:, 1]))\n",
    "sigopt.log_metric('F1score', F1score)\n",
    "sigopt.log_metric('False Positive', fp)\n",
    "sigopt.log_metric('False Negative', fn)\n",
    "sigopt.log_metric('True Positive', tp)\n",
    "sigopt.log_metric('True Negative', tn)\n",
    "sigopt.log_metric('Mean $ Error Fraudulent', error_fraud(prediction, testY, testX['amount']))\n",
    "sigopt.log_metric('Mean $ Error Valid', error_valid(prediction, testY, testX['amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2_idF5O1n56"
   },
   "source": [
    "The [Optimization Loop](https://app.sigopt.com/docs/overview/optimization) is the backbone of using SigOpt.  In the code cell above, you run through the following three simple steps, in a loop\n",
    "- Receive a set of parameters suggestion from SigOpt\n",
    "- Evaluate your model objective metric\n",
    "- Report your model objective metric to SigOpt\n",
    "\n",
    "<img src=\"https://static.sigopt.com/b/7db309215269c8e1d7f88041f283b36b0e0f3884/static/img/optimization-loop.svg\" alt=\"Optimization Loop\"  width=\"250\"/>\n",
    "\n",
    "ðŸš¦ **TO DO - Click the Run hyperlink above. You will be redirected to the the corresponding Run page. Now let's explore the UI and look at how an Experiment comprised of multiple runs differ from a single run like the baseline training run we created earlier.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaNNjJYh1n58"
   },
   "source": [
    "## Multimetric Experimentation\n",
    "\n",
    "Most problems of practical relevance involve two or more competing objectives.  Even in a case like a fraud detection classifier where a clear measure of success is effectively blocking fraudulent transactions while allowing legitimate ones, the classifier inference time should be part of your model evaluation.  So far we've graded the accuracy of the model without worrying about inference time, or how fast we can predict whether a transaction is faudulent or not.  In real life, a model that returns a 99.99% accuracy is useless if it has to churn data for 20 seconds to provide an answer.  SigOpt support [multimetric problems](https://app.sigopt.com/docs/overview/multimetric) where 2 competing metrics can be optimized at the same time, and an additional 50 metrics can be stored.\n",
    "\n",
    "ðŸš¦ **TO DO - Run the next three code cells. The first cell is an update to our experiment definition to include a second objective metric. The scond cell run the optimization loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BssYLK721n59",
    "outputId": "95e400e3-5121-4656-ca70-ca41e266c517"
   },
   "outputs": [],
   "source": [
    "%%experiment\n",
    "'''\n",
    "name: Fraud_Analysis_Multimetric\n",
    "parameters:\n",
    "  - name: min_child_weight\n",
    "    bounds:\n",
    "      min: 1\n",
    "      max: 15\n",
    "    type: int\n",
    "  - name: max_depth\n",
    "    bounds:\n",
    "      min: 2\n",
    "      max: 15\n",
    "    type: int\n",
    "  - name: n_estimators\n",
    "    bounds:\n",
    "      min: 20\n",
    "      max: 400\n",
    "    type: int\n",
    "  - name: learning_rate\n",
    "    bounds:\n",
    "      min: 0.001\n",
    "      max: 1\n",
    "    type: double\n",
    "    transformation: log\n",
    "metrics:\n",
    "  - name: AUPRC\n",
    "    objective: maximize\n",
    "  - name: Inference Time\n",
    "    objective: minimize    \n",
    "observation_budget: 40\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ky0jhDm31n5_",
    "outputId": "fe5d0655-37b1-458f-dadc-c2f38d14876b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%optimize\n",
    "sigopt.log_model('XGboost for Fraud Analysis using multimetric')\n",
    "sigopt.log_dataset(df_path)\n",
    "\n",
    "for key, value in zip(model.get_params().keys(), model.get_params().values()):\n",
    "    sigopt.get_parameter(name=key, default=value)\n",
    "\n",
    "min_child_weight = sigopt.get_parameter('min_child_weight')\n",
    "max_depth = sigopt.get_parameter('max_depth')\n",
    "n_estimators = sigopt.get_parameter('n_estimators')\n",
    "learning_rate = sigopt.get_parameter('learning_rate')\n",
    "\n",
    "\n",
    "model = XGBClassifier(min_child_weight=min_child_weight,\n",
    "                      max_depth=max_depth,\n",
    "                      n_estimators=n_estimators,\n",
    "                      learning_rate=learning_rate,\n",
    "                      random_state = random_state)\n",
    "\n",
    "modelfit = model.fit(trainX,trainY)\n",
    "start = time.time()\n",
    "prediction = modelfit.predict(testX)\n",
    "inferenceTime = time.time() - start\n",
    "F1score = f1_score(testY,prediction)\n",
    "probabilities = modelfit.predict_proba(testX)\n",
    "AUPRC = average_precision_score(testY, probabilities[:, 1])\n",
    "tn, fp, fn, tp = confusion_matrix(testY,prediction).ravel()\n",
    "\n",
    "sigopt.log_metric('AUPRC', average_precision_score(testY, probabilities[:, 1]))\n",
    "sigopt.log_metric('Inference Time', inferenceTime)\n",
    "sigopt.log_metric('F1score', F1score)\n",
    "sigopt.log_metric('False Positive', fp)\n",
    "sigopt.log_metric('False Negative', fn)\n",
    "sigopt.log_metric('True Positive', tp)\n",
    "sigopt.log_metric('True Negative', tn)\n",
    "sigopt.log_metric('Mean $ Error Fraudulent', error_fraud(prediction, testY, testX['amount']))\n",
    "sigopt.log_metric('Mean $ Error Valid', error_valid(prediction, testY, testX['amount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd9gcO0E1n6C"
   },
   "source": [
    "ðŸš¦ **TO DO - Click the link above to open the analysis page. We'll first go over a little bit of theory around multimetric optimization, and the shape of a multimetric solution. Then dive into the analysis once everyone collect enough data to start seeing a pareto frontier**\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn more at your own time\n",
    "## EfficientBERT\n",
    "\n",
    "ðŸš¦ **TO DO [At your own time] - Learn about optimization of efficient BERT by going through the [dashboard](https://app.sigopt.com/guest?guest_token=RDSLBTPPKENPFFAAWDXWGKRUGAJZOPTZZYCRVKCFOZWZZTYL) and watch this [presentation](https://www.youtube.com/watch?v=ZnBYV2h_6RA) by Machine Learning Engineer at SigOpt Meghana Ravikumar**\n",
    "\n",
    "> With the publication of BERT, transfer learning was suddenly accessible for NLP, unlocking a plethora of model zoos and boosting performances for domain specific problems.  Although BERT has accelerated many modeling efforts, its size is limiting. In this talk, we will explore how to reduce the size of BERT while retaining its capacity in the context of English Question Answering tasks. We'll show how scalable hyperparameter optimization can help you tackle difficult modeling problems, draw insights, and make informed decisions.\n",
    "\n",
    "> Our approach encompasses fine-tuning, distillation, architecture search, and hyperparameter optimization at scale. First, we fine-tune BERT on SQUAD 2.0 (our teacher model) and use distillation to compress fine-tuned BERT to a smaller model (our student model). Then, combining SigOpt and Ray, we use multimetric Bayesian optimization at scale to find the optimal architecture for the student model. Finally, we explore the trade-offs of our hyperparameter decisions to draw insights for our student model's architecture. \n",
    "\n",
    "___\n",
    "\n",
    "## Advanced features\n",
    "\n",
    "ðŸš¦ **TO DO [At your own time] - [At your own time] Learn about SigOpt's [advanced features](https://app.sigopt.com/docs/overview/multimetric) for optimization**\n",
    "\n",
    "> The SigOp optimization engine goes beyond traditional hyperparameter tuning packages and methods with numerous advanced features that empower modelers to accelerate model development and solve new optimization problems.\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Documentation\n",
    "\n",
    "ðŸš¦ **TO DO [At your own time] - [At your own time] Learn about SigOpt by looking through the [documentation](https://app.sigopt.com/docs)**\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[O'Reilly] Intro_to_HPO_F9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "11111",
   "language": "python",
   "name": "11111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
